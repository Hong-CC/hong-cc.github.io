{"pages":[{"title":"About me","text":"Hello 大家依舊是一個不務正業，努力證明自己是個笨蛋的失業仔。","link":"/about/index.html"}],"posts":[{"title":"測試","text":"測試發文!!! 預覽看的到我 預覽看不到我","link":"/2020/04/20/%E6%B8%AC%E8%A9%A6/"},{"title":"Katas","text":"這是我在Codewars練習的紀錄。 覺得蠻有趣的，雖然只是個失業的管院仔，練習寫程式也沒用，但還是覺得蠻有趣的。 至於為什麼是Codewars不是LeetCode？因為LeetCode我只寫得出第一題。 5 KyuLand perimeter 題目：https://www.codewars.com/kata/land-perimeter Task：給定的arr裡的X表示一塊1x1的土地，數出土地的周長。 Hint：周長 $=$ 土地塊數 $\\times$ 4 $-$ 相鄰邊數 $\\times$ 2​ （我是google了才知道） My Solution： 123456789101112131415161718def land_perimeter(arr): land = 0 neighbor = 0 for m in range(len(arr)): for n in range(len(arr[0])): if arr[m][n] == &quot;X&quot;: land += 1 # check up and down if m &lt; len(arr) - 1: if arr[m+1][n] == &quot;X&quot;: neighbor += 1 # check left and right if n &lt; len(arr[0]) - 1: if arr[m][n+1] == &quot;X&quot;: neighbor += 1 perimeter = 4*land - 2*neighbor return &quot;Total land perimeter: &quot;+ str(perimeter) Sum of Pairs 題目：https://www.codewars.com/kata/sum-of-pairs Task：給定一串整數列和一個整數（和），求出數列中加起來等於「和」的兩個數。 舉例來說：$([3, 11, 7, 5], 10)$ → 應該得到$[3, 7]$，因為$3+7=10$ Hint：這一題有在LeetCode上解過（唯一寫得出來的QQ的），原先用for迴圈硬尻，但運算時間會太長，所以利用python內dictory索引的特性，讓「差」變成key，「出現過的數字」變成value，以上面舉的例子來說，第一個數字3進入dic後dic = {7:3}，當for輪到7的時候，因為7已經是dic的索引，所以if的判斷式會變False，就會執行else底下的內容，回傳答案。 1234567def sum_pairs(ints, s): dic = {} for num in ints: if num not in dic: dic[s-num] = num else: return [dic[num], num] 6 Kyu","link":"/2020/11/25/Katas/"},{"title":"Regression Analysis Note","text":"碩二上修回歸分析，其中幾個章節的筆記。 Chapter 8: Indicator Variable8.1 The General Concept of Indicator Variables O A B AB D1 1 0 0 0 D2 0 1 0 0 D3 0 0 1 0 Why not O A B AB Type 1 2 3 4 資料沒有差，但寫成迴歸式，等於是用一個$\\beta$來刻劃四維度的血型，實際上的做法應該把四維度區分開來，各別用$\\beta$來捕捉。 Example 8.3 Hint：如果要選多個type，一般會選擇資料筆數較多的那個類別當作基礎值，因為其他的類別都要跟基礎值比，所以我們會希望基礎值先估對，而有比較多的資料，比較可能估的準。 Example 8.5 如果我們將一筆資料切成M等分，A資料可以跑出X影響Y的$\\beta$；B資料也可以跑出X影響Y的$\\beta$我們想知道這M個$\\beta$到底有沒有差異。$$y = \\beta_{0m} + \\beta_{1m}x + \\varepsilon, m=1,2 \\cdots M$$F-test的邏輯，因為$\\beta_1$需要放所有的資料去跑迴歸，但我們把迴歸的係數用indicator的方式拆開。$$y = \\beta_{0m}+\\beta_1x +\\varepsilon \\y = \\beta_1x+\\varepsilon+\\beta_{01}+\\beta_21_{group2}$$ Chapter 9: Regression with Panel DataFundmantal Cross-sectional data 橫斷面資料 (data collected on several individuals/units at one point in time) Time series data (data collected on one individual/unit over several time periods) 時間序列與panel data有很大的不同，最大的不同是，時間序列單一時點，只會有一個觀測值，像是天文資料，觀測不同時點的地球狀況，同一個時點只會有一個地球。 Panel data are repeated corss-sections over time. 從資料結構去想像，最直覺的想法是Panel data是二維的資料，過去可能只有一個方向的資料，現在多加入了時間這個方向。 e.g 過去是第一個病人、第二個病人、第三個病人的狀況；Panel data是第一個病人在時間一、第一個病人在時間二等等。 Some jargon… Another term of panel data is longitudinal data. balance pamel: no missing observations unbalance pamel: 有些空格沒有資料 Why are panel data useful? 一般的迴歸比較怕掉參數的問題，像是沒有考慮到某些需要被納入回歸的因素但資料沒有，或者我們觀察不到。 ​ panel data可以解決某些資料的特性，舉例來說在subj的方向會有性別之類的干擾，但如果我們用panel data的方式處理，因為性別不會隨時間改變，所以性別這個因素，在panel data裡就會被消掉。 Fixed Effect Examople (Week8_1 Panel Regression.pdf) 討論啤酒稅跟酒駕致死率的關係，如果不考慮panel data會得出啤酒稅與酒駕致死率正相關的問題。 這是因為如果我們甚麼都不管，就直接把全部資料全都丟進去跑回歸，等於是假設每個地方(州)每一年的道路品質、飲酒文化都是一樣的，沒有靠慮到每個州年度不同的差異，而這些”背景東西”會對我們做出來的結論造成干擾。 而用panel data是我們利用二維的資料消除掉某些誤差，所以我們會認為panel data model會長這樣：$$FatalityRate_n=\\beta_0+\\beta_1\\cdot BeerTax_{it} +\\beta_2 \\cdot Z_i + \\mu_{it}$$這邊的$Z_i$指的就是那些州別的差異(並假設這些州別的效應，不受時間的影響)，但現在我們遇到的問題是這些影響因素觀察不到，the math consider 1988 and 1982$$FatalityRatei_{1988} = β0 + β_1BeerTax{i,1988} + β2Z_i + u{i1988} \\FatalityRatei_{1982} = β0 + β_1BeerTax{i,1982} + β2Z_i + u{i,1982}$$Suppose E($\\mu_i$|$BeerTax_{it}$,$Z_i$) = 0 因為我們觀察不到$Z_i$所以一二式相減就能得到考慮完州別差異後的結果了，so$$FatalityRatei_{1988} - FatalityRatei_{1982} = \\β1(BeerTax{i,1988}-BeerTax_{i,1982}) + (u_{i1988}-u_{i,1982})$$所以我們不用真的知道那個$Z_i$到底是什麼，總歸我們只知道會有州別的差異，並想辦法把它消掉。 消掉那些可能影響因素的方法 如果跟subj有關，但與時間無關：把$\\alpha_i$對著時間加起來取平均，然後再與原式相減，就能把會造成干擾但看不到的$\\alpha_i$拿掉。 如果跟時間有關，但與subj無關： 跟兩個都有關 → 概念都是把無關的那個方向加起來 Some Theory 在什麼情況下，panel data會順利運作？ Random Effect Fixed effects model VS Random effects model$$Y_{it} = \\beta_1X_{it}+\\alpha_i+u_i$$Fixed effects： 把$\\alpha_i$當作個別的常數估計。但當i很大的時候，或是$\\alpha_i$之間會互相影響時，要個別估各個$\\alpha_i$就會變得比較困難，Random effects：把$\\alpha_i$視作一個分配，將要估的$i$降為2維$\\alpha_i \\sim N(\\mu_i, \\sigma^2_i)$ which one should we choose? depends on 個別樣本能不能被當作從大母體隨機抽樣的樣本 (random sample) $E(\\varepsilon_i X_i) = 0$ if yes: random, if no: fixed T 跟 N 的關係 for large T and small N，沒太大的差別 for large T and large N，random effects好一些 Example$$Y_{it} = \\beta_1 + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it}$$ i: i : th cross-sectional unit, $i = 1, \\dots, N$ t: t : th time period, $ t = 1, \\dots, T$ OLS Regression直接把所有東西都拿來跑回歸，如下面的式子$$Y_{it} = \\beta_1 + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it}$$在這個式子裡，error term捕捉了所有東西，忽略了時間等其他因素。 Fixed Effects Models with Dummy Variable 會因為假設的不同而有不同的模型。 Different intercepts for different individuals $\\beta_{1i}$, but does not vary over time 截距會因為不同的individual而不同，但不隨時間改變 If the number of individuals in $N = 4$$$Y_{it} = \\beta_{1i} + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it} \\Y_{it} = \\alpha_1 + \\alpha_2D_{2i} + \\alpha_3D_{3i} + \\alpha_4D_{4i} + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it}$$Different intercepts for different time period instead $\\beta_{1t}$ 截距因時間不同而不同 If the number of time periods is $T = 20$$$Y_{it} = \\beta_{1t} + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it} \\Y_{it} = \\alpha_1 + \\alpha_2D_{2i} + \\alpha_3D_{3i} + \\alpha_4D_{4i} +\\lambda_1 + \\lambda_2D_{2t} + \\lambda_3D_{3t} + \\dots +\\lambda_{20}D_{20t} + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it}$$Different intercepts for different individual AND time periods $\\beta_{1it}$ 截距因idividuals 與時間不同而不同 For $N=4 \\ and \\ T = 20$$$\\begin{align}Y_{it} &amp;= \\beta_{1it} + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it} \\Y_{it} &amp;= \\alpha_1 + \\alpha_2D_{2i} + \\alpha_3D_{3i} + \\alpha_4D_{4i} \\ &amp; + \\lambda_1 + \\lambda_2D_{2t} + \\lambda_3D_{3t} + \\dots +\\lambda_{20}D_{20t} + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it}\\end{align}$$Both intercepts and slopes varies over individuals For $N=4$$$\\begin{align}Y_{it} &amp;= \\alpha_1 + \\alpha_2D_{2i} + \\alpha_3D_{3i} + \\alpha_4D_{4i} + \\beta_2X_{2it} + \\beta_3X_{3it} \\&amp;+ \\gamma_1D_{2i}X{2it} + \\gamma_2D_{2i}X_{3it}\\ &amp; +\\gamma_3D_{3i}X_{2it} +\\gamma_{4}D_{3i}X_{3it} \\ &amp; + \\gamma_{5}D_{4i}X_{2it} + \\gamma_6D_{4it}X_{3it} + u_{it}\\end{align}$$Both intercepts and slope varies ovet time periods Random Effects Models$$Y_{it} = \\beta_{1i} + \\beta_2X_{2it} + \\beta_3X_{3it} + u_{it}$$ intercepts/effects $\\beta_{1i}$ 被視為$E(\\beta_{1i}) = \\beta_1$的隨機變數，所以個別individual的截距可以表達為$$\\beta_{1i} = \\beta_1 + \\varepsilon_i \\ \\ \\ \\ i = 1, \\dots,N \\where \\ E(\\varepsilon_i) = 0 \\ and \\ Var(\\varepsilon_i) = \\sigma^2_\\varepsilon$$所以原式可以改寫成$$Y_{it} = \\beta_{1i} + \\beta_2X_{2it} + \\beta_3X_{3it} + \\varepsilon_i + u_{it} \\Y_{it} = \\beta_{1i} + \\beta_2X_{2it} + \\beta_3X_{it} + w_{it}$$其中$\\varepsilon_i$指的是只對$i$有關的觀測誤差，對所有$t$都是一樣的 $w_{it} = \\varepsilon_{i} + u_{it}$已經把所有的effect併到裡頭了 而組合誤差$w_{it}$須滿足下列假設： $E(w_{it}) = 0$ $Var(w_{it}) = \\sigma^2_\\varepsilon+\\sigma^2_u$ $Corr(w_{it}, w_{is})= \\frac{\\sigma^2_\\varepsilon}{\\sigma^2_\\varepsilon+\\sigma^2_u}$ 由於$Var \\ not \\ homo \\ and \\ Corr \\neq0$，所以使用HAC方法做調整。 Chapter 10: Variable Selection &amp; Model Building10.1 Introduction 需要注意的是，並沒有所謂最好的模型，而是端專案想要達到的目標和解決的問題。 Two “conflicting” goal: 盡可能多的regressors。 太多的regressors卻會導致$\\hat y$的變異增加，而且在收集資料時需要更多的成本。 舉例來說，像是ML或是無母數的分析方法，就是選擇「盡可能多的regressors」因為他們更重視的是用來做預測，對於變數的顯不顯著其實就不是那麼重要，至於需不需要做降維度則取決於專案的目標，或是電腦能不能處理。 原則上來說： 放太少參數，會變成偏誤估計量 放太多參數，變異會變大 10.2 Computational Techniques for Variable Selection forward selection backward selection stepwise regression Forward selection 從什麼都沒有開始選。$$y = \\beta_0 + \\varepsilon \\step \\ 1: \\y = \\beta_0 + \\beta_1X_1 + \\varepsilon \\y = \\beta_0 + \\beta_2X_2 + \\varepsilon \\y = \\beta_0 + \\beta_3X_3 + \\varepsilon \\y = \\beta_0 + \\beta_4X_4 + \\varepsilon \\$$從這四個model裡挑比原本model好的，如果有兩個以上比原本的好就挑最好的，可以透過檢定$\\beta_1 = 0$的F test的方式，找出最顯著(F值最大的)的。如此一步一步挑下來，直到得不到任何顯著的model。 相較於backward，forward比較容易停在一個參數比較少，比較精簡的model，因為forward是從什麼都沒有開始挑。 Backward selection 跟Forward相反，從full model開始選。 Stepwise regression 原則是forward，但後續的步驟會是forward與backward共存。 Summary 一般而言，會把上面這三個方法都試試看，如果真的有個解釋變數很重要，那應該不管用哪種方式，都會出現在最後的模型裡，算是一種比較保險的選變數的方式。 Principal Component Analysis 假如我們有一筆資料n x k，n是資料筆數，k是變數，但k很大，我們需要進行降維，來幫助我們進一步分析，但丟掉變數，一定會丟掉資訊，但我們該如何選擇要丟掉的變數，來減少降維的損失。 PCA 會遇到的兩個問題： 原本的$X_1, X_2, \\dots ,X_P$會變成$PC_1, PC_2, \\dots , PC_p$但這些PC是透過X組合而成的，並無法解釋，所以如果我們想做的事，是要好好解釋這些X怎麼影響y，PCA可能就無法使用。 因為PCA是針對X進行降維，雖然保留了X的資訊，但不能保證這些資訊能用於解釋y。 Chapter 11: Validation of Regression Model Model adequacy checking，是確定模型的基礎假設沒有太離譜的違反。 Model validation，是確保模型可以用。 Chapter 12: Nonlinear Regression如何判斷是不是非線性模型？對模型取期望值，接著對$\\beta$偏微分，如果結果還有參數存在，那就是非線性模型，但這種方式有個缺點是我們必須先知道模型長怎樣。","link":"/2020/05/31/Regression/"},{"title":"MCMC Note","text":"紀錄一下蒙地卡羅馬可夫算法(MCMC)免得期末考又忘記怎麼做。 但先記錄Gibbs (因為我還是搞懂MH倒底怎麼做的，希望能快點補上QQ) Gibbs Sample也可以看做是Metropolis-Hasting Sampler的一種特例。 使用時機當聯合分布$f(x,y)$未知或很醜很難從聯合機率分佈抽樣，但我們卻知道單一變數的條件機率，即已知$f(x|y)$與$f(y|x)$，在這種情況下，就能使用Gibbs來幫助我們抽到$x, y $的聯合分佈。 大概念是輪流抽x跟y，先給定y抽x，再給定剛剛抽到的x抽新的y，如此反覆，得到最終的結果。 演算法下面的pseudo code 來自這裡 1234567Algorithm GibbsSampling(X, Y)　Y[0] = random initialize a distribution　for i = 1 to N　　generate X[i] from P(X | Y[i-1])　　generate Y[i] from P(Y | X[i]) 　return {X[N], Y[N]}End Algorithm 當然也可以輕鬆的把上面的算法由2個隨機變數擴充成k個，概念一樣，當要抽$j$的時候就給定除了$j$以外的隨機變數，pseudo code如下。 123456789101112Algorithm GibbsSampling(X[1...k])　X = random initialize a distribution　for i = 1 to N　　generate X&apos;[1] from X and P(X[1] | X[2], ..., X[k]) ...　　generate X&apos;[j] from X and P(X[i] | X[1], ..., X[j-1], X[j+1],...,X[k]) ...　　generate X&apos;[k] from X and P(X[k] | X[1], ..., X[k-1]) X = X&apos; end　return XEnd Algorithm 實作(這其實是作業QQ) Given an efficient method for generating nine uniform points on (0,1) conditional on the event than no two of them are within 0.1 of each other. Using Gibbs Sampler, the algorithm is implemented as follows. Step 1: set $x_1、x_2 \\dots x_9 = 0.1、0.2 \\dots 0.9$ Step 2:$x_1|x_2\\dots x_9 \\sim U(0,x_2-0.1)$$x_2|x_1 \\ x_3\\dots x_9 \\sim U(x_1+0.1,x_3-0.1)$$x_3|x_1 \\ x_2 \\ x_4 \\dots x_9 \\sim U(x_2+0.1,x_4-0.1)$$\\vdots$$x_9|x_1 \\dots x_8 \\sim U(x_8+0.1, 1)$ Step 3: Repeat Step 2 N times. Repeat 10 time. The result will be1[0.00920446 0.12711574 0.24654891 0.40492615 0.53389484 0.6616597 0.77638536 0.88252739 0.98980808] Here is the python code:123456789101112131415161718192021222324import numpy as npdef Gibbs(trans, init, n): for i in range(n): for j in range(len(init)): x_p = trans(init, j) return x_pdef test_trans(x, d): a = 0 if d &gt; 0: a = x[d-1]+0.1 b = 1 if d &lt; 8: b = x[d+1]-0.1 ans = x ans[d] = np.random.uniform()*(b-a)+a return ans # simulatenp.random.seed(0)init = np.linspace(0.1, 1, 9)simu = Gibbs(trans= test_trans, init=init, n = 100)print(simu)","link":"/2020/06/02/MCMC/"}],"tags":[{"name":"test","slug":"test","link":"/tags/test/"},{"name":"Statistics","slug":"Statistics","link":"/tags/Statistics/"}],"categories":[]}